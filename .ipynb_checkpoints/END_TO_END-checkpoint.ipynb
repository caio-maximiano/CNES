{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from dateutil.relativedelta import *\n",
    "import zipfile, sys, os, psycopg2, wget\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"PFC\") \\\n",
    "        .config(\"spark.jars\", \"file:///C:/SparkLocal/spark-3.1.3-bin-hadoop2.7/jars/mysql-connector-java-8.0.30\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"C:/Users/013503631/Documents/Caio/PFC/CNES_ETL_PFC-JUPYTER/CSV_FILES/\"\n",
    "zip_path = \"C:/Users/013503631/Documents/Caio/PFC/CNES_ETL_PFC-JUPYTER/ZIP_FILES/\"\n",
    "curated_path = \"C:/Users/013503631/Documents/Caio/PFC/CNES_ETL_PFC-JUPYTER/CURATED_FILES/\"\n",
    "\n",
    "use_date = datetime.datetime.now() + relativedelta(months=-2)\n",
    "ano_mes = use_date.strftime(\"%Y%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(periodo):\n",
    "    \n",
    "    ftp_path = f\"ftp://ftp.datasus.gov.br/cnes/BASE_DE_DADOS_CNES_{ano_mes}.ZIP\"\n",
    "    \n",
    "    #Verifica se arquivo já não existe para iniciar downlaod do servidor FTP\n",
    "    if os.path.exists(zip_path + f\"BASE_DE_DADOS_CNES_{ano_mes}.ZIP\") == False: \n",
    "        try:\n",
    "            wget.download(ftp_path, zip_path) #Acessa servidor FTP para baixar arquivo\n",
    "        except:\n",
    "            print(\"O erro\", sys.exc_info()[0], \"ocorreu.\")\n",
    "    else:\n",
    "        print(f\"O arquivos zip de {ano_mes} já existe\")\n",
    "        \n",
    "def extract_csv(periodo):\n",
    "    \n",
    "    final_path = f\"{csv_path}{periodo}\"\n",
    "    \n",
    "    #Verifica se a pasta destino não existe para iniciar processo\n",
    "    if os.path.exists(final_path) == False:\n",
    "        try:\n",
    "            os.chdir(zip_path)\n",
    "            for file in os.listdir(zip_path):\n",
    "                if zipfile.is_zipfile(file): \n",
    "                    with zipfile.ZipFile(file) as item: \n",
    "                       item.extractall(final_path)\n",
    "                    os.remove(file)\n",
    "        except:\n",
    "            print(\"O erro\", sys.exc_info()[0], \"ocorreu.\")     \n",
    "    else:\n",
    "        print(f\"A pasta de destino {ano_mes} já existe\")       \n",
    "        \n",
    "def get_csv(file_name, periodo):\n",
    "    \n",
    "    file_path = f\"{csv_path}{periodo}/{file_name}{periodo}.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = spark.read\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"delimiter\", \";\")\\\n",
    "                .csv(f\"{file_path}\")\n",
    "        return df\n",
    "    except:\n",
    "        print(\"O erro\", sys.exc_info()[0], \"ocorreu.\")\n",
    "        \n",
    "def write_curated_file(df, file_name, ano_mes):\n",
    "    file_path = f'{curated_path}{file_name}_{ano_mes}'\n",
    "\n",
    "    try:\n",
    "        df.coalesce(1)\\\n",
    "                .write\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(f\"{file_path}\")\n",
    "\n",
    "        for partition in os.listdir(f\"{file_path}\"):\n",
    "            if partition.startswith(\"part-\"):\n",
    "                old_name = os.path.join(f\"{file_path}\", partition)\n",
    "                new_name = os.path.join(f\"{file_path}\", f\"{file_name}_{ano_mes}.csv\")\n",
    "                os.rename(old_name, new_name)\n",
    "    \n",
    "    except:\n",
    "        print(\"O erro\", sys.exc_info()[0], \"ocorreu.\")\n",
    "\n",
    "\n",
    "def update_table(table_name, file_name, ano_mes):\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(database=\"postgres\",\n",
    "                                user='postgres', password='system', \n",
    "                                host='localhost', port='5432')\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        new_file = f'C:/Users/013503631/Documents/Caio/PFC/CNES_ETL_PFC-JUPYTER/CURATED_FILES/{file_name}_{ano_mes}/{file_name}_{ano_mes}.csv'\n",
    "\n",
    "        truncate = f'TRUNCATE TABLE {table_name}'\n",
    "        cursor.execute(truncate)\n",
    "        \n",
    "        update = f'''COPY {table_name} FROM '{new_file}' DELIMITER ',' CSV HEADER;'''\n",
    "        cursor.execute(update)\n",
    "        print(f\"Table {table_name} updated\")\n",
    "    \n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error in update operation\", error)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if conn:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1178d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_zip(ano_mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119bff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_csv(ano_mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3dc4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_cnes = ['tbCargaHorariaSus', 'rlEstabServClass', 'tbAtividadeProfissional', 'tbClassificacaoServico',\\\n",
    "                 'tbDadosProfissionalSus', 'tbEstabelecimento', 'tbMunicipio']\n",
    "\n",
    "for file in arquivos_cnes:\n",
    "    file = get_csv(file, ano_mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_sp = [tbEstabelecimento.CO_MUNICIPIO_GESTOR == tbMunicipio.CO_MUNICIPIO,\n",
    "           tbEstabelecimento.CO_ESTADO_GESTOR == 35] #SP\n",
    "\n",
    "cond_serv = [rlEstabServClass.CO_SERVICO == tbClassificacaoServico.CO_SERVICO_ESPECIALIZADO,\n",
    "        rlEstabServClass.CO_CLASSIFICACAO == tbClassificacaoServico.CO_CLASSIFICACAO_SERVICO]\n",
    "\n",
    "estab_munic = tbEstabelecimento.join(tbMunicipio, cond_sp)\n",
    "\n",
    "\n",
    "df_serv = rlEstabServClass\\\n",
    "            .join(tbClassificacaoServico, cond_serv)\\\n",
    "            .join(estab_munic, rlEstabServClass.CO_UNIDADE == estab_munic.CO_UNIDADE)\\\n",
    "            .select(rlEstabServClass.CO_UNIDADE,\n",
    "                rlEstabServClass.CO_SERVICO,\n",
    "                rlEstabServClass.CO_CLASSIFICACAO,\n",
    "                tbClassificacaoServico.DS_CLASSIFICACAO_SERVICO)\\\n",
    "            .withColumn('DATA_INGESTAO', to_date(current_timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = tbCargaHorariaSus\\\n",
    "    .join(tbAtividadeProfissional, tbCargaHorariaSus.CO_CBO ==tbAtividadeProfissional.CO_CBO)\\\n",
    "    .join(estab_munic, tbCargaHorariaSus.CO_UNIDADE == estab_munic.CO_UNIDADE)\\\n",
    "    .join(tbDadosProfissionalSus, tbCargaHorariaSus.CO_PROFISSIONAL_SUS == tbDadosProfissionalSus.CO_PROFISSIONAL_SUS)\\\n",
    "    .select(tbCargaHorariaSus.CO_UNIDADE,\n",
    "            tbCargaHorariaSus.CO_PROFISSIONAL_SUS,\n",
    "            tbDadosProfissionalSus.NO_PROFISSIONAL,\n",
    "            tbCargaHorariaSus.CO_CBO,\n",
    "            tbCargaHorariaSus.TP_SUS_NAO_SUS,            \n",
    "            tbAtividadeProfissional.DS_ATIVIDADE_PROFISSIONAL,\n",
    "            estab_munic.NO_FANTASIA,\n",
    "            estab_munic.NO_BAIRRO,\n",
    "            estab_munic.NO_MUNICIPIO,\n",
    "            estab_munic.CO_MUNICIPIO,\n",
    "            estab_munic.CO_SIGLA_ESTADO,\n",
    "            estab_munic.CO_CEP,\n",
    "            )\\\n",
    "    .withColumn('DATA_INGESTAO', to_date(current_timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c43fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_curated_file(df_final, 'curated_estabelecimentos', ano_mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_curated_file(df_serv, 'curated_servicos', ano_mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d501a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_table('curated_servicos', 'curated_servicos', ano_mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec58ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_table('curated_estabelecimentos', 'curated_estabelecimentos', ano_mes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
